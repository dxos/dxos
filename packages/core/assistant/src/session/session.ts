//
// Copyright 2025 DXOS.org
//

import { type AiError, AiLanguageModel, type AiResponse, type AiTool } from '@effect/ai';
import { Chunk, Effect, Option, Queue, type Schema, Stream } from 'effect';

import {
  type AiInputPreprocessingError,
  AiParser,
  AiPreprocessor,
  type AiToolNotFoundError,
  type ToolExecutionService,
  type ToolResolverService,
  callTool,
  getToolCalls,
} from '@dxos/ai';
import { todo } from '@dxos/debug';
import { Obj } from '@dxos/echo';
import { TracingService } from '@dxos/functions';
import { log } from '@dxos/log';
import { type ContentBlock, DataType } from '@dxos/schema';

import { type AiAssistantError } from '../errors';

import { formatSystemPrompt, formatUserPrompt } from './format';
import { GenerationObserver } from './observer';
import { type ToolkitParams, createToolkit } from './toolkit';

export type AiSessionRunError = AiError.AiError | AiInputPreprocessingError | AiToolNotFoundError | AiAssistantError;

export type AiSessionRunRequirements<Tools extends AiTool.Any> =
  | AiLanguageModel.AiLanguageModel
  | AiTool.ToHandler<Tools>
  | ToolExecutionService
  | ToolResolverService
  | TracingService;

export type AiSessionRunParams<Tools extends AiTool.Any> = ToolkitParams<Tools> & {
  prompt: string;
  system?: string;
  history?: DataType.Message[];
  objects?: Obj.Any[];
  observer?: GenerationObserver;
};

export type AiSessionOptions = {};

/**
 * Contains message history, tools, current context.
 * Current context means the state of the app, time of day, and other contextual information.
 * It makes requests to the model, its a state machine.
 * It keeps track of the current goal.
 * It manages the context window.
 * Tracks the success criteria of reaching the goal, exposing metrics (stretch).
 * Could be run locally in the app or remotely.
 * Could be personal or shared.
 */
export class AiSession {
  // TODO(dmaretskyi): Unify with observer and merge queues into a single stream.
  public readonly messageQueue = Effect.runSync(Queue.unbounded<DataType.Message>());
  public readonly blockQueue = Effect.runSync(Queue.unbounded<Option.Option<ContentBlock.Any>>());
  public readonly eventQueue = Effect.runSync(Queue.unbounded<AiResponse.Part>());

  /** Prevents concurrent execution of session. */
  private readonly _semaphore = Effect.runSync(Effect.makeSemaphore(1));

  /**
   * Prior history from queue.
   * NOTE: The conversation should evolve into supporting a git-like graph of messages.
   */
  private _history: DataType.Message[] = [];

  /**
   * Pending messages for this session (incl. the current prompt).
   */
  private _pending: DataType.Message[] = [];

  constructor(private readonly _options: AiSessionOptions = {}) {}

  /**
   * Runs the AI model loop interacting with tools and artifacts.
   * @returns The messages generated by the session, including the user's prompt.
   */
  // TODO(dmaretskyi): Toolkit context doesn't get added to the effect type.
  run = <Tools extends AiTool.Any>({
    prompt,
    system: systemTemplate,
    history = [],
    objects = [],
    blueprints = [],
    toolIds = [],
    toolkit,
    observer = GenerationObserver.noop(),
  }: AiSessionRunParams<Tools>): Effect.Effect<
    DataType.Message[],
    AiSessionRunError,
    AiSessionRunRequirements<Tools>
  > =>
    Effect.gen(this, function* () {
      const now = Date.now();
      let toolCount = 0;

      // Reset.
      this._history = [...history];
      this._pending = [];

      // Create toolkit.
      const toolkitHandlers = yield* createToolkit({ toolkit, toolIds, blueprints });

      // Generate system prompt.
      // TODO(budon): Dynamically resolve template variables here.
      const system = yield* formatSystemPrompt({ system: systemTemplate, blueprints, objects });

      const pending = this._pending;
      const messageQueue = this.messageQueue;
      const submitMessage = Effect.fnUntraced(function* (message: DataType.Message) {
        pending.push(message);
        yield* messageQueue.offer(message);
        yield* observer.onMessage(message);
        yield* TracingService.emitConverationMessage(message);
        return message;
      });

      const promptMessage = yield* submitMessage(yield* formatUserPrompt({ prompt, history }));

      //
      // Tool call loop.
      //
      do {
        log.info('request', {
          prompt: promptMessage,
          system: { snippet: createSnippet(system), length: system.length },
          toolkit: Object.values(toolkitHandlers.tools).map((tool: AiTool.Any) => tool.name),
          pending: this._pending.length,
          history: this._history.length,
          objects: objects?.length ?? 0,
          blueprints: blueprints?.length ?? 0,
        });

        //
        // Make the request.
        //
        const prompt = yield* AiPreprocessor.preprocessAiInput([...this._history, ...this._pending]);
        const blocks = yield* AiLanguageModel.streamText({
          prompt,
          system,
          toolkit: toolkitHandlers,
          // TODO(burdon): Check if this bug has been fixed and update deps/patches?
          // TODO(burdon): Despite this flag, the model still calls tools.
          //  Flag is only used in generateText (not streamText); patch and submit bug.
          //  https://github.com/Effect-TS/effect/blob/main/packages/ai/ai/src/AiLanguageModel.ts#L401
          disableToolCallResolution: true,
        }).pipe(
          AiParser.parseResponse({
            onBlock: (block) =>
              Effect.all([this.blockQueue.offer(Option.some(block)), observer.onBlock(block)], {
                discard: true,
              }),
            onPart: (part) =>
              Effect.all([this.eventQueue.offer(part), observer.onPart(part)], {
                discard: true,
              }),
          }),
          Stream.runCollect,
          Effect.map(Chunk.toArray),
        );

        // Signal to stream consumers that message blocks are complete.
        // Allows for coordination between the block and message queues
        //   to prevent the streaming blocks from being rendered twice when the message is produced.
        yield* this.blockQueue.offer(Option.none());

        // Create response message.
        const response = yield* submitMessage(
          Obj.make(DataType.Message, {
            created: new Date().toISOString(),
            sender: { role: 'assistant' },
            blocks,
          }),
        );

        // Parse response for tool calls.
        const toolCalls = getToolCalls(response);
        if (toolCalls.length === 0) {
          break;
        }

        // TODO(burdon): Retry backend errors?
        const toolResults = yield* Effect.forEach(toolCalls, (toolCall) => {
          toolCount++;
          return callTool(toolkitHandlers, toolCall).pipe(
            Effect.provide(
              TracingService.layerSubframe((context) => ({
                ...context,
                parentMessage: response.id,
                toolCallId: toolCall.toolCallId,
              })),
            ),
          );
        });

        // Add to queue and continue loop.
        yield* submitMessage(
          Obj.make(DataType.Message, {
            created: new Date().toISOString(),
            sender: { role: 'user' },
            blocks: toolResults,
          }),
        );
      } while (true);

      // Summary.
      yield* submitMessage(
        Obj.make(DataType.Message, {
          created: new Date().toISOString(),
          sender: { role: 'assistant' },
          blocks: [
            {
              _tag: 'summary',
              message: 'Success',
              duration: Date.now() - now,
              toolCalls: toolCount,
              // TODO(burdon): Get token count.
            },
          ],
        }),
      );

      // Signals to stream consumers that the session has completed and no more messages are coming.
      yield* Queue.shutdown(this.messageQueue);
      yield* Queue.shutdown(this.blockQueue);
      yield* Queue.shutdown(this.eventQueue);

      log('done', { pending: this._pending.length });
      return this._pending;
    }).pipe(this._semaphore.withPermits(1), Effect.withSpan('AiSession.run'));

  // TODO(burdon): Implement.
  async runStructured<S extends Schema.Schema.AnyNoContext>(
    _schema: S,
    _options: AiSessionRunParams<AiTool.Any>,
  ): Promise<Schema.Schema.Type<S>> {
    return todo();
    // const parser = structuredOutputParser(schema);
    // const result = await this.run({
    //   ...options,
    //   executableTools: [...(options.executableTools ?? []), parser.tool],
    // });
    // return parser.getResult(result);
  }
}

const createSnippet = (text: string, len = 32) =>
  text.length <= len * 2 ? text : [text.slice(0, len), '...', text.slice(-len)].join('');
